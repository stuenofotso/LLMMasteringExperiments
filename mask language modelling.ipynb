{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (4.45.1)\n",
      "Requirement already satisfied: filelock in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from transformers) (1.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: torch in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers\n",
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, AutoTokenizer, AutoModelForMaskedLM\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.special import softmax\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/steve/miniconda3/envs/llm/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = tokenizer.mask_token\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'want', 'to', '[MASK]', 'pizza', 'for', 'tonight', '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = f\"I want to {mask} pizza for tonight.\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs = tokenizer(text=sentence, return_tensors=\"pt\")\n",
    "output = model(**encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28996"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = output.logits.detach().numpy()[0]\n",
    "mask_logits = logits[tokens.index(mask)+1]\n",
    "mask_logits.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "have 0.2572901\n",
      "get 0.17849621\n",
      "eat 0.15555479\n",
      "make 0.11422449\n",
      "order 0.09823056\n"
     ]
    }
   ],
   "source": [
    "confidence_score = softmax(mask_logits)\n",
    "print(confidence_score.sum())\n",
    "for i in np.argsort(confidence_score)[::-1][:5]: #arg correspond Ã  l'indice du token dans le vocabulaire [::-1] permet d'inverser l'ordre de l'array\n",
    "    pred_token = tokenizer.decode(i) #recupere le mot associÃ© au token\n",
    "    score = confidence_score[i]\n",
    "    \n",
    "    print(pred_token, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1138, 1243, 3940, 1294, 1546])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(confidence_score)[::-1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input):\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "    encoded_inputs = tokenizer(text=input, return_tensors=\"pt\")\n",
    "    output = model(**encoded_inputs)\n",
    "    logits = output.logits.detach().numpy()[0]\n",
    "    mask_logits = logits[tokens.index(mask)+1]\n",
    "    confidence_score = softmax(mask_logits)\n",
    "    return [{\"token\":tokenizer.decode(i), \"score\":confidence_score[i]} for i in np.argsort(confidence_score)[::-1][:5]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'token': 'No', 'score': 0.22621737},\n",
       " {'token': 'Yes', 'score': 0.15105757},\n",
       " {'token': 'NO', 'score': 0.04536177},\n",
       " {'token': 'no', 'score': 0.038355332},\n",
       " {'token': 'None', 'score': 0.021622486}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(f\"Is John Kennedy (JFK) dead? Answer (yes/no):{mask}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'token': '5', 'score': 0.17081268},\n",
       " {'token': '6', 'score': 0.14476721},\n",
       " {'token': '7', 'score': 0.119548105},\n",
       " {'token': '8', 'score': 0.10006619},\n",
       " {'token': '9', 'score': 0.07793571}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(f\"Using my mathematic calculator, 2+3=5, 1+8={mask}, 6+7=13\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
